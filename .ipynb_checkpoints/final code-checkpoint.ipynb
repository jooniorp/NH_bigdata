{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0LJXy-09jsP"
   },
   "source": [
    "# 1. MEGRE 파일을 만들기 위해 NH_CONTEST_NW_FC_STK_IEM_IFO, NH_CONTEST_STK_DT_QUT, NH_CONTEST_NHDATA_STK_DD_IFO 3개의 파일을 업로드 한 후 각 파일별로 이름을 매칭함.\n",
    "# 2. 첫번째파일에서는 종목별 상장주식총수량을 가져와서 시가총액을 시계열 데이터로 만들기 위해 종목종가와 곱해서 구하는 방법으로 사용하기 위해 해당 컬럼만 가져오도록 하였음.\n",
    "# 3. 그리고 두번째, 세번째 파일은 첫번째 파일과 함께 세가지 CSV 파일에 공통으로 있는 tck_iem_cd를 기준으로 merge를 하였으며, 두번째, 세번째 파일은 bse_dt도 비교하여 가져오도록 하여 총 77794개의 시계열 데이터를 하나의 csv 파일로 모으는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "vQJb8neOOyD2",
    "outputId": "66e66093-cfc9-4940-f152-38638cb54bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드된 파일: ['NH_CONTEST_NHDATA_STK_DD_IFO (1).csv', 'NH_CONTEST_NW_FC_STK_IEM_IFO (1).csv', 'NH_CONTEST_STK_DT_QUT (1).csv']\n",
      "1번 파일: NH_CONTEST_NW_FC_STK_IEM_IFO (1).csv\n",
      "2번 파일: NH_CONTEST_STK_DT_QUT (1).csv\n",
      "3번 파일: NH_CONTEST_NHDATA_STK_DD_IFO (1).csv\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_c97d9fcf-b2a9-40e3-8597-81f001cb647c\", \"merge_output.csv\", 18360162)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일이 성공적으로 저장되었습니다: merge_output.csv\n",
      "df1 tck_iem_cd 유니크 값: ['AA' 'AAL' 'AAN' ... 'ZWS' 'ZYME' 'ZYXI']\n",
      "filtered_df2 tck_iem_cd 유니크 값: ['AAL' 'AAOI' 'AAPB' ... 'ZIM' 'ZM' 'ZS']\n",
      "filtered_df3 tck_iem_cd 유니크 값: ['AAL' 'AAOI' 'AAPB' ... 'ZIM' 'ZM' 'ZS']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# CSV 파일 업로드\n",
    "#uploaded = files.upload()\n",
    "# 업로드된 파일 목록 확인\n",
    "file_names = list(uploaded.keys())\n",
    "print(\"업로드된 파일:\", file_names)\n",
    "\n",
    "# 파일 이름 설정\n",
    "big_contest_file_1 = None\n",
    "big_contest_file_2 = None\n",
    "big_contest_file_3 = None\n",
    "\n",
    "# 파일 이름 매칭\n",
    "for file_name in file_names:\n",
    "    if 'NH_CONTEST_NW_FC_STK_IEM_IFO.csv' in file_name:\n",
    "        big_contest_file_1 = file_name\n",
    "    elif 'NH_CONTEST_STK_DT_QUT.csv' in file_name:\n",
    "        big_contest_file_2 = file_name\n",
    "    elif 'NH_CONTEST_NHDATA_STK_DD_IFO.csv' in file_name:\n",
    "        big_contest_file_3 = file_name\n",
    "\n",
    "# 파일 이름 확인\n",
    "print(\"1번 파일:\", big_contest_file_1)\n",
    "print(\"2번 파일:\", big_contest_file_2)\n",
    "print(\"3번 파일:\", big_contest_file_3)\n",
    "\n",
    "# CSV 파일 읽기 (인코딩 지정)\n",
    "df1 = pd.read_csv(big_contest_file_1, encoding='ISO-8859-1')  # 첫 번째 파일 데이터프레임\n",
    "df2 = pd.read_csv(big_contest_file_2, encoding='ISO-8859-1')  # 두 번째 파일 데이터프레임\n",
    "df3 = pd.read_csv(big_contest_file_3, encoding='ISO-8859-1')  # 세 번째 파일 데이터프레임\n",
    "\n",
    "# 공백 제거 및 대문자 변환\n",
    "df1['tck_iem_cd'] = df1['tck_iem_cd'].astype(str).str.strip().str.upper()\n",
    "df2['tck_iem_cd'] = df2['tck_iem_cd'].astype(str).str.strip().str.upper()\n",
    "df3['tck_iem_cd'] = df3['tck_iem_cd'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# bse_dt 공백 제거\n",
    "df2['bse_dt'] = df2['bse_dt'].astype(str).str.strip()\n",
    "df3['bse_dt'] = df3['bse_dt'].astype(str).str.strip()\n",
    "\n",
    "# df1에서 tck_iem_cd와 ltg_tot_stk_qty 가져오기\n",
    "ltg_tot_stk_qty_df = df1[['tck_iem_cd', 'ltg_tot_stk_qty']]\n",
    "\n",
    "# df2와 df3에서 공통 bse_dt와 tck_iem_cd 찾기\n",
    "common_keys_df2_df3 = pd.merge(df2[['bse_dt', 'tck_iem_cd']], df3[['bse_dt', 'tck_iem_cd']], on=['bse_dt', 'tck_iem_cd'])\n",
    "filtered_df2 = df2[df2.set_index(['bse_dt', 'tck_iem_cd']).index.isin(common_keys_df2_df3.set_index(['bse_dt', 'tck_iem_cd']).index)]\n",
    "filtered_df3 = df3[df3.set_index(['bse_dt', 'tck_iem_cd']).index.isin(common_keys_df2_df3.set_index(['bse_dt', 'tck_iem_cd']).index)]\n",
    "\n",
    "# df2에 ltg_tot_stk_qty를 tck_iem_cd 기준으로 결합\n",
    "filtered_df2 = filtered_df2.merge(ltg_tot_stk_qty_df, on='tck_iem_cd', how='left')\n",
    "\n",
    "# 새로운 컬럼 mkt_pr_tot_amt 생성 (ltg_tot_stk_qty와 IEM_END_PR 곱하기)\n",
    "filtered_df2['mkt_pr_tot_amt'] = filtered_df2['ltg_tot_stk_qty'] * filtered_df2['iem_end_pr']\n",
    "\n",
    "# ltg_tot_stk_qty 컬럼 삭제\n",
    "filtered_df2.drop(columns=['ltg_tot_stk_qty'], inplace=True)\n",
    "\n",
    "# 최종 병합: df2와 df3\n",
    "merged_df = pd.merge(filtered_df2, filtered_df3, on=['bse_dt', 'tck_iem_cd'], suffixes=('_df2', '_df3'))\n",
    "\n",
    "# 특정 조건에 맞는 값 업데이트\n",
    "merged_df.loc[(merged_df['tck_iem_cd'] == 'ZVRA') & (merged_df['bse_dt'] == '20240802'),\n",
    "               ['iem_ong_pr', 'iem_hi_pr', 'iem_low_pr', 'iem_end_pr']] = 6.3\n",
    "\n",
    "# 결과를 새로운 CSV 파일로 저장\n",
    "output_file = 'merge_output.csv'  # 결과 파일 이름\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# 결과 파일 다운로드\n",
    "files.download(output_file)\n",
    "\n",
    "print(\"파일이 성공적으로 저장되었습니다:\", output_file)\n",
    "print(\"df1 tck_iem_cd 유니크 값:\", df1['tck_iem_cd'].unique())\n",
    "print(\"filtered_df2 tck_iem_cd 유니크 값:\", filtered_df2['tck_iem_cd'].unique())\n",
    "print(\"filtered_df3 tck_iem_cd 유니크 값:\", filtered_df3['tck_iem_cd'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WECne3C_A5U5"
   },
   "source": [
    "# 1. 파일 BIG_CONTEST_DATA_ETF_HOLDINGS.csv을 업로드한 후 SEC_TP가 ST인 개별종목에 대한 ETF 종목들을 가져오도록 하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "-HW4ByBMkLUD",
    "outputId": "cc605b6b-74c0-4f76-fa29-311cf3b85d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드된 파일: ['NH_CONTEST_DATA_ETF_HOLDINGS.csv']\n",
      "  etf_tck_cd tck_iem_cd   mkt_vlu            fc_sec_eng_nm fc_sec_krl_nm  \\\n",
      "0       AAPB       AAPL  36858666                     AAPL            애플   \n",
      "3       AMDL        AMD   6530355                      AMD          에이엠디   \n",
      "4       CLOU       TWLO  15400502           TWILIO INC - A          트윌리오   \n",
      "5       CLOU       AKAM  15954631  AKAMAI TECHNOLOGIES INC          아카마이   \n",
      "6       CLOU        BOX  16465312        BOX INC - CLASS A            박스   \n",
      "\n",
      "   stk_qty  wht_pct sec_tp  \n",
      "0   215737   66.778     ST  \n",
      "3    36558   66.718     ST  \n",
      "4   254933    4.266     ST  \n",
      "5   157173    4.419     ST  \n",
      "6   598521    4.561     ST  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_542956f5-6d26-44b7-bd0d-38548d8acad2\", \"only_st_etf.csv\", 2090622)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링된 파일이 성공적으로 저장되었습니다: only_st_etf.csv\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# 파일 업로드\n",
    "#uploaded = files.upload()\n",
    "\n",
    "# 업로드된 파일 목록 확인\n",
    "file_names = list(uploaded.keys())\n",
    "print(\"업로드된 파일:\", file_names)\n",
    "\n",
    "# 파일 이름 설정\n",
    "data_file = file_names[0]  # 데이터 파일\n",
    "\n",
    "# CSV 파일 읽기 (인코딩 지정)\n",
    "df = pd.read_csv(data_file, encoding='cp949')  # 데이터프레임\n",
    "\n",
    "# sec_tp 컬럼의 값이 'ST'인 행만 남기기\n",
    "df_filtered = df[df['sec_tp'] == 'ST']\n",
    "\n",
    "# 결합 결과 확인\n",
    "print(df_filtered.head())\n",
    "\n",
    "# 결과를 새로운 CSV 파일로 저장\n",
    "output_file = 'only_st_etf.csv'  # 결과 파일 이름\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "# 결과 파일 다운로드\n",
    "files.download(output_file)\n",
    "\n",
    "print(\"필터링된 파일이 성공적으로 저장되었습니다:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYjLq8M5Wbvk"
   },
   "source": [
    "# 1. merge_output.csv 파일을 바탕으로 필요한 컬럼들을 가져오고, bse_dt를 기준으로 정렬하는데, 종목별로 그룹화해서 정렬하도록 코드 작성\n",
    "# 2. bse_dt를 기준으로 정렬된 결과를 토대로 shift를 이용해서 비거래일을 제외하고 1일전, 5일전, 14일전, 1일후, 5일후, 14일후 대비 변화량을 기준일로 나누어서 기울기 데이터를 뽑아냄.\n",
    "# 3. NATR의 경우 기준일로부터 N일치 TRUE RANGE를 뽑아서 평균가를 기준으로 정규화를 1차적으로 진행합니다.\n",
    "# 4. 기울기값과 NATR값들을 최종적으로 0과 1사이로 MIN-MAX 정규화 진행합니다.\n",
    "# 5. 거래대금 및 시가총액 데이터도 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-JMz-DoDXA7",
    "outputId": "5a23a372-36b8-4460-8a58-81801679e697"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-54b0d9d6c36c>:115: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('tck_iem_cd').apply(calculate_trends).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화된 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('merge_output.csv')\n",
    "\n",
    "# 데이터 정렬\n",
    "df.sort_values(by=['tck_iem_cd', 'bse_dt'], inplace=True)\n",
    "\n",
    "# TR 계산\n",
    "def calculate_tr(group):\n",
    "    group['Previous Close'] = group['iem_end_pr'].shift(1)\n",
    "    group['TR'] = group[['iem_hi_pr', 'iem_low_pr', 'Previous Close']].apply(\n",
    "        lambda x: max(x['iem_hi_pr'] - x['iem_low_pr'],\n",
    "                      abs(x['iem_hi_pr'] - x['Previous Close']),\n",
    "                      abs(x['iem_low_pr'] - x['Previous Close'])), axis=1)\n",
    "    return group\n",
    "\n",
    "# NATR 계산 (여러 기간)\n",
    "def calculate_natr_multiple(group, periods=[7, 14, 30]):\n",
    "    group = calculate_tr(group)  # TR 계산\n",
    "    for period in periods:\n",
    "        group[f'NATR_{period}'] = (group['TR'].rolling(window=period).mean() / group['iem_end_pr'])\n",
    "    return group\n",
    "\n",
    "\n",
    "# 반올림 함수\n",
    "def round_values(group, columns):\n",
    "    for column in columns:\n",
    "        group[column] = group[column].round(4)  # 셋째 자리에서 반올림\n",
    "    return group\n",
    "\n",
    "# 그룹화 및 기울기 계산\n",
    "def calculate_trends(group):\n",
    "    # 인덱스를 bse_dt로 설정\n",
    "    group.set_index('bse_dt', inplace=True)\n",
    "\n",
    "    # 기울기 계산 (비거래일 고려)\n",
    "    group['slope_tco_avg_eal_pls_1d'] = (group['tco_avg_eal_pls'].shift(1) - group['tco_avg_eal_pls']) / 1\n",
    "    group['slope_tco_avg_eal_pls_5d'] = (group['tco_avg_eal_pls'].shift(5) - group['tco_avg_eal_pls']) / 5\n",
    "    group['slope_tco_avg_eal_pls_14d'] = (group['tco_avg_eal_pls'].shift(14) - group['tco_avg_eal_pls']) / 14\n",
    "    group['slope_tco_avg_pft_rt_1d'] = (group['tco_avg_pft_rt'].shift(1) - group['tco_avg_pft_rt']) / 1\n",
    "    group['slope_tco_avg_pft_rt_5d'] = (group['tco_avg_pft_rt'].shift(5) - group['tco_avg_pft_rt']) / 5\n",
    "    group['slope_tco_avg_pft_rt_14d'] = (group['tco_avg_pft_rt'].shift(14) - group['tco_avg_pft_rt']) / 14\n",
    "    group['slope_lss_ivo_rt_1d'] = (group['lss_ivo_rt'].shift(1) - group['lss_ivo_rt']) / 1\n",
    "    group['slope_lss_ivo_rt_5d'] = (group['lss_ivo_rt'].shift(5) - group['lss_ivo_rt']) / 5\n",
    "    group['slope_lss_ivo_rt_14d'] = (group['lss_ivo_rt'].shift(14) - group['lss_ivo_rt']) / 14\n",
    "    group['slope_ifw_act_cnt_1d'] = (group['ifw_act_cnt'].shift(1) - group['ifw_act_cnt']) / 1\n",
    "    group['slope_ifw_act_cnt_5d'] = (group['ifw_act_cnt'].shift(5) - group['ifw_act_cnt']) / 5\n",
    "    group['slope_ifw_act_cnt_14d'] = (group['ifw_act_cnt'].shift(14) - group['ifw_act_cnt']) / 14\n",
    "    group['slope_ofw_act_cnt_1d'] = (group['ofw_act_cnt'].shift(1) - group['ofw_act_cnt']) / 1\n",
    "    group['slope_ofw_act_cnt_5d'] = (group['ofw_act_cnt'].shift(5) - group['ofw_act_cnt']) / 5\n",
    "    group['slope_ofw_act_cnt_14d'] = (group['ofw_act_cnt'].shift(14) - group['ofw_act_cnt']) / 14\n",
    "    group['slope_vw_tgt_cnt_1d'] = (group['vw_tgt_cnt'].shift(1) - group['vw_tgt_cnt']) / 1\n",
    "    group['slope_vw_tgt_cnt_5d'] = (group['vw_tgt_cnt'].shift(5) - group['vw_tgt_cnt']) / 5\n",
    "    group['slope_vw_tgt_cnt_14d'] = (group['vw_tgt_cnt'].shift(14) - group['vw_tgt_cnt']) / 14\n",
    "    group['slope_rgs_tgt_cnt_1d'] = (group['rgs_tgt_cnt'].shift(1) - group['rgs_tgt_cnt']) / 1\n",
    "    group['slope_rgs_tgt_cnt_5d'] = (group['rgs_tgt_cnt'].shift(5) - group['rgs_tgt_cnt']) / 5\n",
    "    group['slope_rgs_tgt_cnt_14d'] = (group['rgs_tgt_cnt'].shift(14) - group['rgs_tgt_cnt']) / 14\n",
    "    group['slope_trd_cst_1d'] = (group['trd_cst'].shift(1) - group['trd_cst']) / 1\n",
    "    group['slope_trd_cst_5d'] = (group['trd_cst'].shift(5) - group['trd_cst']) / 5\n",
    "    group['slope_trd_cst_14d'] = (group['trd_cst'].shift(14) - group['trd_cst']) / 14\n",
    "    # NaN 값을 -9999로 대체\n",
    "    # NaN 값을 -99로 대체\n",
    " #   group[['slope_tco_avg_eal_pls_1d', 'slope_tco_avg_eal_pls_5d', 'slope_tco_avg_eal_pls_14d']] = group[['slope_tco_avg_eal_pls_1d', 'slope_tco_avg_eal_pls_5d', 'slope_tco_avg_eal_pls_14d']].fillna(-9999)\n",
    " #   group[['slope_tco_avg_pft_rt_1d', 'slope_tco_avg_pft_rt_5d', 'slope_tco_avg_pft_rt_14d']] = group[['slope_tco_avg_pft_rt_1d', 'slope_tco_avg_pft_rt_5d', 'slope_tco_avg_pft_rt_14d']].fillna(-9999)\n",
    " #   group[['slope_lss_ivo_rt_1d', 'slope_lss_ivo_rt_5d', 'slope_lss_ivo_rt_14d']] = group[['slope_lss_ivo_rt_1d', 'slope_lss_ivo_rt_5d', 'slope_lss_ivo_rt_14d']].fillna(-9999)\n",
    " #   group[['slope_ifw_act_cnt_1d', 'slope_ifw_act_cnt_5d', 'slope_ifw_act_cnt_14d']] = group[['slope_ifw_act_cnt_1d', 'slope_ifw_act_cnt_5d', 'slope_ifw_act_cnt_14d']].fillna(-9999)\n",
    " #   group[['slope_ofw_act_cnt_1d', 'slope_ofw_act_cnt_5d', 'slope_ofw_act_cnt_14d']] = group[['slope_ofw_act_cnt_1d', 'slope_ofw_act_cnt_5d', 'slope_ofw_act_cnt_14d']].fillna(-9999)\n",
    " #   group[['slope_vw_tgt_cnt_1d', 'slope_vw_tgt_cnt_5d', 'slope_vw_tgt_cnt_14d']] = group[['slope_vw_tgt_cnt_1d', 'slope_vw_tgt_cnt_5d', 'slope_vw_tgt_cnt_14d']].fillna(-9999)\n",
    " #   group[['slope_rgs_tgt_cnt_1d', 'slope_rgs_tgt_cnt_5d', 'slope_rgs_tgt_cnt_14d']] = group[['slope_rgs_tgt_cnt_1d', 'slope_rgs_tgt_cnt_5d', 'slope_rgs_tgt_cnt_14d']].fillna(-9999)\n",
    " #   group[['slope_trd_cst_1d', 'slope_trd_cst_5d', 'slope_trd_cst_14d']] = group[['slope_trd_cst_1d', 'slope_trd_cst_5d', 'slope_trd_cst_14d']].fillna(-9999)\n",
    "\n",
    "    # 미래 기울기 계산\n",
    "    group['future_slope_tco_avg_eal_pls_1d'] = (group['tco_avg_eal_pls'].shift(-1) - group['tco_avg_eal_pls']) / 1\n",
    "    group['future_slope_tco_avg_eal_pls_5d'] = (group['tco_avg_eal_pls'].shift(-5) - group['tco_avg_eal_pls']) / 5\n",
    "    group['future_slope_tco_avg_eal_pls_14d'] = (group['tco_avg_eal_pls'].shift(-14) - group['tco_avg_eal_pls']) / 14\n",
    "    group['future_slope_tco_avg_pft_rt_1d'] = (group['tco_avg_pft_rt'].shift(-1) - group['tco_avg_pft_rt']) / 1\n",
    "    group['future_slope_tco_avg_pft_rt_5d'] = (group['tco_avg_pft_rt'].shift(-5) - group['tco_avg_pft_rt']) / 5\n",
    "    group['future_slope_tco_avg_pft_rt_14d'] = (group['tco_avg_pft_rt'].shift(-14) - group['tco_avg_pft_rt']) / 14\n",
    "    group['future_slope_lss_ivo_rt_1d'] = (group['lss_ivo_rt'].shift(-1) - group['lss_ivo_rt']) / 1\n",
    "    group['future_slope_lss_ivo_rt_5d'] = (group['lss_ivo_rt'].shift(-5) - group['lss_ivo_rt']) / 5\n",
    "    group['future_slope_lss_ivo_rt_14d'] = (group['lss_ivo_rt'].shift(-14) - group['lss_ivo_rt']) / 14\n",
    "    group['future_slope_ifw_act_cnt_1d'] = (group['ifw_act_cnt'].shift(-1) - group['ifw_act_cnt']) / 1\n",
    "    group['future_slope_ifw_act_cnt_5d'] = (group['ifw_act_cnt'].shift(-5) - group['ifw_act_cnt']) / 5\n",
    "    group['future_slope_ifw_act_cnt_14d'] = (group['ifw_act_cnt'].shift(-14) - group['ifw_act_cnt']) / 14\n",
    "    group['future_slope_ofw_act_cnt_1d'] = (group['ofw_act_cnt'].shift(-1) - group['ofw_act_cnt']) / 1\n",
    "    group['future_slope_ofw_act_cnt_5d'] = (group['ofw_act_cnt'].shift(-5) - group['ofw_act_cnt']) / 5\n",
    "    group['future_slope_ofw_act_cnt_14d'] = (group['ofw_act_cnt'].shift(-14) - group['ofw_act_cnt']) / 14\n",
    "    group['future_slope_vw_tgt_cnt_1d'] = (group['vw_tgt_cnt'].shift(-1) - group['vw_tgt_cnt']) / 1\n",
    "    group['future_slope_vw_tgt_cnt_5d'] = (group['vw_tgt_cnt'].shift(-5) - group['vw_tgt_cnt']) / 5\n",
    "    group['future_slope_vw_tgt_cnt_14d'] = (group['vw_tgt_cnt'].shift(-14) - group['vw_tgt_cnt']) / 14\n",
    "    group['future_slope_rgs_tgt_cnt_1d'] = (group['rgs_tgt_cnt'].shift(-1) - group['rgs_tgt_cnt']) / 1\n",
    "    group['future_slope_rgs_tgt_cnt_5d'] = (group['rgs_tgt_cnt'].shift(-5) - group['rgs_tgt_cnt']) / 5\n",
    "    group['future_slope_rgs_tgt_cnt_14d'] = (group['rgs_tgt_cnt'].shift(-14) - group['rgs_tgt_cnt']) / 14\n",
    "    group['future_slope_trd_cst_1d'] = (group['trd_cst'].shift(-1) - group['trd_cst']) / 1\n",
    "    group['future_slope_trd_cst_5d'] = (group['trd_cst'].shift(-5) - group['trd_cst']) / 5\n",
    "    group['future_slope_trd_cst_14d'] = (group['trd_cst'].shift(-14) - group['trd_cst']) / 14\n",
    "    # 미래 슬로프 관련 NaN 값을 -99로 대체\n",
    "#    group[['future_slope_tco_avg_eal_pls_1d', 'future_slope_tco_avg_eal_pls_5d', 'future_slope_tco_avg_eal_pls_14d']] = group[['future_slope_tco_avg_eal_pls_1d', 'future_slope_tco_avg_eal_pls_5d', 'future_slope_tco_avg_eal_pls_14d']].fillna(-9999)\n",
    "#    group[['future_slope_tco_avg_pft_rt_1d', 'future_slope_tco_avg_pft_rt_5d', 'future_slope_tco_avg_pft_rt_14d']] = group[['future_slope_tco_avg_pft_rt_1d', 'future_slope_tco_avg_pft_rt_5d', 'future_slope_tco_avg_pft_rt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_lss_ivo_rt_1d', 'future_slope_lss_ivo_rt_5d', 'future_slope_lss_ivo_rt_14d']] = group[['future_slope_lss_ivo_rt_1d', 'future_slope_lss_ivo_rt_5d', 'future_slope_lss_ivo_rt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_ifw_act_cnt_1d', 'future_slope_ifw_act_cnt_5d', 'future_slope_ifw_act_cnt_14d']] = group[['future_slope_ifw_act_cnt_1d', 'future_slope_ifw_act_cnt_5d', 'future_slope_ifw_act_cnt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_ofw_act_cnt_1d', 'future_slope_ofw_act_cnt_5d', 'future_slope_ofw_act_cnt_14d']] = group[['future_slope_ofw_act_cnt_1d', 'future_slope_ofw_act_cnt_5d', 'future_slope_ofw_act_cnt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_vw_tgt_cnt_1d', 'future_slope_vw_tgt_cnt_5d', 'future_slope_vw_tgt_cnt_14d']] = group[['future_slope_vw_tgt_cnt_1d', 'future_slope_vw_tgt_cnt_5d', 'future_slope_vw_tgt_cnt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_rgs_tgt_cnt_1d', 'future_slope_rgs_tgt_cnt_5d', 'future_slope_rgs_tgt_cnt_14d']] = group[['future_slope_rgs_tgt_cnt_1d', 'future_slope_rgs_tgt_cnt_5d', 'future_slope_rgs_tgt_cnt_14d']].fillna(-9999)\n",
    "#    group[['future_slope_trd_cst_1d', 'future_slope_trd_cst_5d', 'future_slope_trd_cst_14d']] = group[['future_slope_trd_cst_1d', 'future_slope_trd_cst_5d', 'future_slope_trd_cst_14d']].fillna(-9999)\n",
    "\n",
    "    # NATR 계산 추가 (7일, 14일, 30일)\n",
    "    group = calculate_natr_multiple(group)  # NATR 계산\n",
    "\n",
    "    return group.reset_index()  # 인덱스 초기화 및 bse_dt 컬럼 유지\n",
    "\n",
    "# 그룹화 적용\n",
    "result = df.groupby('tck_iem_cd').apply(calculate_trends).reset_index(drop=True)\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "final_result = result[['tck_iem_cd', 'bse_dt', 'mkt_pr_tot_amt', 'trd_cst',\n",
    "    'slope_tco_avg_eal_pls_1d', 'slope_tco_avg_eal_pls_5d', 'slope_tco_avg_eal_pls_14d',\n",
    "    'future_slope_tco_avg_eal_pls_1d', 'future_slope_tco_avg_eal_pls_5d', 'future_slope_tco_avg_eal_pls_14d',\n",
    "    'slope_tco_avg_pft_rt_1d', 'slope_tco_avg_pft_rt_5d', 'slope_tco_avg_pft_rt_14d',\n",
    "    'future_slope_tco_avg_pft_rt_1d', 'future_slope_tco_avg_pft_rt_5d', 'future_slope_tco_avg_pft_rt_14d',\n",
    "    'slope_lss_ivo_rt_1d', 'slope_lss_ivo_rt_5d', 'slope_lss_ivo_rt_14d',\n",
    "    'future_slope_lss_ivo_rt_1d', 'future_slope_lss_ivo_rt_5d', 'future_slope_lss_ivo_rt_14d',\n",
    "    'slope_ifw_act_cnt_1d', 'slope_ifw_act_cnt_5d', 'slope_ifw_act_cnt_14d',\n",
    "    'future_slope_ifw_act_cnt_1d', 'future_slope_ifw_act_cnt_5d', 'future_slope_ifw_act_cnt_14d',\n",
    "    'slope_ofw_act_cnt_1d', 'slope_ofw_act_cnt_5d', 'slope_ofw_act_cnt_14d',\n",
    "    'future_slope_ofw_act_cnt_1d', 'future_slope_ofw_act_cnt_5d', 'future_slope_ofw_act_cnt_14d',\n",
    "    'slope_vw_tgt_cnt_1d', 'slope_vw_tgt_cnt_5d', 'slope_vw_tgt_cnt_14d',\n",
    "    'future_slope_vw_tgt_cnt_1d', 'future_slope_vw_tgt_cnt_5d', 'future_slope_vw_tgt_cnt_14d',\n",
    "    'slope_rgs_tgt_cnt_1d', 'slope_rgs_tgt_cnt_5d', 'slope_rgs_tgt_cnt_14d',\n",
    "    'future_slope_rgs_tgt_cnt_1d', 'future_slope_rgs_tgt_cnt_5d', 'future_slope_rgs_tgt_cnt_14d',\n",
    "    'slope_trd_cst_1d', 'slope_trd_cst_5d', 'slope_trd_cst_14d',\n",
    "    'future_slope_trd_cst_1d', 'future_slope_trd_cst_5d', 'future_slope_trd_cst_14d', 'NATR_7', 'NATR_14', 'NATR_30'\n",
    "]]\n",
    "\n",
    "# 정규화 함수\n",
    "def min_max_normalize(df, columns):\n",
    "    normalized_df = df.copy()\n",
    "    for column in columns:\n",
    "        # NaN 값 제외\n",
    "        filtered_column = normalized_df[column][normalized_df[column].notna()]\n",
    "\n",
    "        if not filtered_column.empty:\n",
    "            min_val = filtered_column.min()\n",
    "            max_val = filtered_column.max()\n",
    "            normalized_df[column] = normalized_df[column].apply(\n",
    "                lambda x: (x - min_val) / (max_val - min_val) if max_val != min_val and pd.notna(x) else x\n",
    "            )\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "# 정규화할 열 리스트\n",
    "columns_to_normalize = ['mkt_pr_tot_amt', 'trd_cst',\n",
    "    'slope_tco_avg_eal_pls_1d', 'slope_tco_avg_eal_pls_5d', 'slope_tco_avg_eal_pls_14d',\n",
    "    'future_slope_tco_avg_eal_pls_1d', 'future_slope_tco_avg_eal_pls_5d', 'future_slope_tco_avg_eal_pls_14d',\n",
    "    'slope_tco_avg_pft_rt_1d', 'slope_tco_avg_pft_rt_5d', 'slope_tco_avg_pft_rt_14d',\n",
    "    'future_slope_tco_avg_pft_rt_1d', 'future_slope_tco_avg_pft_rt_5d', 'future_slope_tco_avg_pft_rt_14d',\n",
    "    'slope_lss_ivo_rt_1d', 'slope_lss_ivo_rt_5d', 'slope_lss_ivo_rt_14d',\n",
    "    'future_slope_lss_ivo_rt_1d', 'future_slope_lss_ivo_rt_5d', 'future_slope_lss_ivo_rt_14d',\n",
    "    'slope_ifw_act_cnt_1d', 'slope_ifw_act_cnt_5d', 'slope_ifw_act_cnt_14d',\n",
    "    'future_slope_ifw_act_cnt_1d', 'future_slope_ifw_act_cnt_5d', 'future_slope_ifw_act_cnt_14d',\n",
    "    'slope_ofw_act_cnt_1d', 'slope_ofw_act_cnt_5d', 'slope_ofw_act_cnt_14d',\n",
    "    'future_slope_ofw_act_cnt_1d', 'future_slope_ofw_act_cnt_5d', 'future_slope_ofw_act_cnt_14d',\n",
    "    'slope_vw_tgt_cnt_1d', 'slope_vw_tgt_cnt_5d', 'slope_vw_tgt_cnt_14d',\n",
    "    'future_slope_vw_tgt_cnt_1d', 'future_slope_vw_tgt_cnt_5d', 'future_slope_vw_tgt_cnt_14d',\n",
    "    'slope_rgs_tgt_cnt_1d', 'slope_rgs_tgt_cnt_5d', 'slope_rgs_tgt_cnt_14d',\n",
    "    'future_slope_rgs_tgt_cnt_1d', 'future_slope_rgs_tgt_cnt_5d', 'future_slope_rgs_tgt_cnt_14d',\n",
    "    'slope_trd_cst_1d', 'slope_trd_cst_5d', 'slope_trd_cst_14d',\n",
    "    'future_slope_trd_cst_1d', 'future_slope_trd_cst_5d', 'future_slope_trd_cst_14d', 'NATR_7', 'NATR_14', 'NATR_30'\n",
    "]\n",
    "\n",
    "# 정규화 수행\n",
    "final_result_normalized = min_max_normalize(final_result, columns_to_normalize)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = 'slopes_about_normalized.csv'  # 정규화된 결과 파일 이름\n",
    "final_result_normalized.to_csv(output_file, index=False)\n",
    "\n",
    "# 파일 다운로드\n",
    "#files.download(output_file)  # 파일 다운로드 기능은 환경에 따라 다를 수 있습니다.\n",
    "print(\"정규화된 CSV 파일로 저장되었습니다.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
